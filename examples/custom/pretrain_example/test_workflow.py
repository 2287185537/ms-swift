# Copyright (c) Alibaba, Inc. and its affiliates.
"""
æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹
éªŒè¯åˆ†è¯å™¨è®­ç»ƒ â†’ æ¨¡å‹åˆå§‹åŒ– â†’ æ³¨å†Œ â†’ æ¨ç†çš„å®Œæ•´æµç¨‹
"""

import os
import sys
import shutil
import tempfile
from pathlib import Path


def test_tokenizer_training():
    """æµ‹è¯•åˆ†è¯å™¨è®­ç»ƒ"""
    print("\n" + "="*60)
    print("æµ‹è¯• 1: åˆ†è¯å™¨è®­ç»ƒ")
    print("="*60)
    
    from train_tokenizer import train_custom_tokenizer
    
    with tempfile.TemporaryDirectory() as tmpdir:
        tokenizer_dir = os.path.join(tmpdir, "test_tokenizer")
        
        try:
            tokenizer = train_custom_tokenizer(
                input_files=[],  # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
                output_dir=tokenizer_dir,
                vocab_size=1000,  # å°è¯æ±‡è¡¨ç”¨äºå¿«é€Ÿæµ‹è¯•
                min_frequency=1
            )
            
            print(f"\nâœ“ åˆ†è¯å™¨è®­ç»ƒæˆåŠŸ")
            print(f"  è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
            print(f"  PAD token ID: {tokenizer.pad_token_id}")
            print(f"  BOS token ID: {tokenizer.bos_token_id}")
            print(f"  EOS token ID: {tokenizer.eos_token_id}")
            
            # æµ‹è¯•ç¼–ç è§£ç 
            text = "æµ‹è¯•æ–‡æœ¬"
            encoded = tokenizer.encode(text)
            decoded = tokenizer.decode(encoded)
            print(f"  ç¼–ç æµ‹è¯•: '{text}' -> {encoded[:10]}... -> '{decoded}'")
            
            return True, tokenizer_dir
            
        except Exception as e:
            print(f"\nâœ— åˆ†è¯å™¨è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None


def test_model_initialization(tokenizer_dir):
    """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–ï¼ˆä»åˆ†è¯å™¨å¯¼å…¥å‚æ•°ï¼‰"""
    print("\n" + "="*60)
    print("æµ‹è¯• 2: æ¨¡å‹åˆå§‹åŒ–ï¼ˆä»åˆ†è¯å™¨å¯¼å…¥å‚æ•°ï¼‰")
    print("="*60)
    
    from transformers import AutoTokenizer
    from custom_model import CustomModelConfig, CustomModelForCausalLM
    
    with tempfile.TemporaryDirectory() as tmpdir:
        model_dir = os.path.join(tmpdir, "test_model")
        
        try:
            # åŠ è½½åˆ†è¯å™¨
            print("åŠ è½½åˆ†è¯å™¨...")
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir, trust_remote_code=True)
            
            # ä»åˆ†è¯å™¨å¯¼å…¥å‚æ•°
            vocab_size = tokenizer.vocab_size
            pad_token_id = tokenizer.pad_token_id or 0
            bos_token_id = tokenizer.bos_token_id or 1
            eos_token_id = tokenizer.eos_token_id or 2
            
            print(f"\nä»åˆ†è¯å™¨å¯¼å…¥çš„å‚æ•°:")
            print(f"  vocab_size: {vocab_size}")
            print(f"  pad_token_id: {pad_token_id}")
            print(f"  bos_token_id: {bos_token_id}")
            print(f"  eos_token_id: {eos_token_id}")
            
            # åˆ›å»ºæ¨¡å‹é…ç½®ï¼ˆä½¿ç”¨åˆ†è¯å™¨å‚æ•°ï¼‰
            config = CustomModelConfig(
                vocab_size=vocab_size,  # ä»åˆ†è¯å™¨å¯¼å…¥
                hidden_size=256,  # å°æ¨¡å‹ç”¨äºå¿«é€Ÿæµ‹è¯•
                num_hidden_layers=2,
                num_attention_heads=4,
                intermediate_size=1024,
                max_position_embeddings=512,
                pad_token_id=pad_token_id,  # ä»åˆ†è¯å™¨å¯¼å…¥
                bos_token_id=bos_token_id,  # ä»åˆ†è¯å™¨å¯¼å…¥
                eos_token_id=eos_token_id,  # ä»åˆ†è¯å™¨å¯¼å…¥
            )
            
            # åˆå§‹åŒ–æ¨¡å‹
            print("\nåˆå§‹åŒ–æ¨¡å‹...")
            model = CustomModelForCausalLM(config)
            
            # éªŒè¯ä¸€è‡´æ€§
            assert model.config.vocab_size == tokenizer.vocab_size, \
                f"è¯æ±‡è¡¨å¤§å°ä¸ä¸€è‡´: æ¨¡å‹={model.config.vocab_size}, åˆ†è¯å™¨={tokenizer.vocab_size}"
            
            print(f"âœ“ æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            print(f"  æ¨¡å‹è¯æ±‡è¡¨å¤§å°: {model.config.vocab_size}")
            print(f"  åˆ†è¯å™¨è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
            print(f"  âœ“ éªŒè¯é€šè¿‡: è¯æ±‡è¡¨å¤§å°ä¸€è‡´")
            
            # ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨
            os.makedirs(model_dir, exist_ok=True)
            model.save_pretrained(model_dir)
            tokenizer.save_pretrained(model_dir)
            
            print(f"  æ¨¡å‹å·²ä¿å­˜åˆ°: {model_dir}")
            
            return True, model_dir
            
        except Exception as e:
            print(f"\nâœ— æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None


def test_model_forward(model_dir):
    """æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­"""
    print("\n" + "="*60)
    print("æµ‹è¯• 3: æ¨¡å‹å‰å‘ä¼ æ’­")
    print("="*60)
    
    import torch
    from transformers import AutoTokenizer
    from custom_model import CustomModelForCausalLM
    
    try:
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        print("åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
        model = CustomModelForCausalLM.from_pretrained(model_dir)
        tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        test_text = "ä½ å¥½ï¼Œä¸–ç•Œï¼"
        print(f"\næµ‹è¯•æ–‡æœ¬: {test_text}")
        
        encoded = tokenizer.encode(test_text, return_tensors="pt")
        print(f"ç¼–ç å shape: {encoded.shape}")
        
        with torch.no_grad():
            outputs = model(encoded)
            logits = outputs.logits
        
        print(f"è¾“å‡º logits shape: {logits.shape}")
        print(f"é¢„æœŸ shape: (1, {encoded.shape[1]}, {model.config.vocab_size})")
        
        assert logits.shape == (1, encoded.shape[1], model.config.vocab_size), \
            f"è¾“å‡º shape ä¸æ­£ç¡®: {logits.shape}"
        
        print(f"\nâœ“ å‰å‘ä¼ æ’­æµ‹è¯•æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"\nâœ— å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_generation(model_dir):
    """æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ"""
    print("\n" + "="*60)
    print("æµ‹è¯• 4: æ–‡æœ¬ç”Ÿæˆ")
    print("="*60)
    
    import torch
    from transformers import AutoTokenizer
    from custom_model import CustomModelForCausalLM
    
    try:
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        print("åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
        model = CustomModelForCausalLM.from_pretrained(model_dir)
        tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
        
        # ç”Ÿæˆæ–‡æœ¬
        prompt = "ä½ å¥½"
        print(f"\næç¤ºè¯: {prompt}")
        
        input_ids = tokenizer.encode(prompt, return_tensors="pt")
        
        with torch.no_grad():
            output_ids = model.generate(
                input_ids,
                max_new_tokens=10,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        print(f"ç”Ÿæˆæ–‡æœ¬: {generated_text}")
        
        print(f"\nâœ“ æ–‡æœ¬ç”Ÿæˆæµ‹è¯•æˆåŠŸ")
        print(f"  æ³¨æ„: ç”±äºæ¨¡å‹æœªè®­ç»ƒï¼Œç”Ÿæˆçš„æ–‡æœ¬æ˜¯éšæœºçš„")
        
        return True
        
    except Exception as e:
        print(f"\nâœ— æ–‡æœ¬ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("="*60)
    print("è‡ªå®šä¹‰é¢„è®­ç»ƒæ¨¡å‹å·¥ä½œæµç¨‹æµ‹è¯•")
    print("="*60)
    print("\næµ‹è¯•æµç¨‹:")
    print("1. è®­ç»ƒè‡ªå®šä¹‰åˆ†è¯å™¨")
    print("2. æ ¹æ®åˆ†è¯å™¨å‚æ•°åˆå§‹åŒ–æ¨¡å‹")
    print("3. æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­")
    print("4. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ")
    print()
    
    results = []
    
    # æµ‹è¯• 1: åˆ†è¯å™¨è®­ç»ƒ
    success, tokenizer_dir = test_tokenizer_training()
    results.append(("åˆ†è¯å™¨è®­ç»ƒ", success))
    
    if not success:
        print("\nåˆ†è¯å™¨è®­ç»ƒå¤±è´¥ï¼Œåç»­æµ‹è¯•è·³è¿‡")
        print_summary(results)
        return
    
    # æµ‹è¯• 2: æ¨¡å‹åˆå§‹åŒ–
    success, model_dir = test_model_initialization(tokenizer_dir)
    results.append(("æ¨¡å‹åˆå§‹åŒ–ï¼ˆå‚æ•°å¯¼å…¥ï¼‰", success))
    
    if not success:
        print("\næ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œåç»­æµ‹è¯•è·³è¿‡")
        print_summary(results)
        return
    
    # æµ‹è¯• 3: å‰å‘ä¼ æ’­
    success = test_model_forward(model_dir)
    results.append(("æ¨¡å‹å‰å‘ä¼ æ’­", success))
    
    # æµ‹è¯• 4: æ–‡æœ¬ç”Ÿæˆ
    success = test_generation(model_dir)
    results.append(("æ–‡æœ¬ç”Ÿæˆ", success))
    
    # æ‰“å°æ€»ç»“
    print_summary(results)


def print_summary(results):
    """æ‰“å°æµ‹è¯•æ€»ç»“"""
    print("\n" + "="*60)
    print("æµ‹è¯•æ€»ç»“")
    print("="*60)
    
    for test_name, success in results:
        status = "âœ“ é€šè¿‡" if success else "âœ— å¤±è´¥"
        print(f"{status}: {test_name}")
    
    total = len(results)
    passed = sum(1 for _, success in results if success)
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å·¥ä½œæµç¨‹éªŒè¯æˆåŠŸï¼")
        print("\nä¸‹ä¸€æ­¥:")
        print("1. è¿è¡Œ bash run_full_example.sh æŸ¥çœ‹å®Œæ•´æµç¨‹")
        print("2. ä½¿ç”¨è‡ªå·±çš„æ•°æ®è®­ç»ƒåˆ†è¯å™¨å’Œæ¨¡å‹")
        print("3. å¼€å§‹é¢„è®­ç»ƒ: bash pretrain.sh")
    else:
        print(f"\nâš  {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    
    print("="*60)


if __name__ == '__main__':
    # ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•
    script_dir = Path(__file__).parent
    os.chdir(script_dir)
    
    # è¿è¡Œæµ‹è¯•
    main()
