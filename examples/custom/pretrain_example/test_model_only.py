# Copyright (c) Alibaba, Inc. and its affiliates.
"""
ç®€åŒ–æµ‹è¯•ï¼šä»…æµ‹è¯•æ¨¡å‹å®šä¹‰å’ŒåŸºæœ¬åŠŸèƒ½ï¼ˆä¸éœ€è¦ tokenizers åŒ…ï¼‰
"""

import torch
from custom_model import CustomModelConfig, CustomModelForCausalLM


def test_model_creation():
    """æµ‹è¯•æ¨¡å‹åˆ›å»º"""
    print("="*60)
    print("æµ‹è¯• 1: æ¨¡å‹åˆ›å»º")
    print("="*60)
    
    try:
        # åˆ›å»ºå°å‹é…ç½®ç”¨äºå¿«é€Ÿæµ‹è¯•
        config = CustomModelConfig(
            vocab_size=1000,
            hidden_size=256,
            num_hidden_layers=2,
            num_attention_heads=4,
            intermediate_size=1024,
            max_position_embeddings=512,
            pad_token_id=0,
            bos_token_id=1,
            eos_token_id=2,
        )
        
        print(f"æ¨¡å‹é…ç½®:")
        print(f"  vocab_size: {config.vocab_size}")
        print(f"  hidden_size: {config.hidden_size}")
        print(f"  num_hidden_layers: {config.num_hidden_layers}")
        print(f"  num_attention_heads: {config.num_attention_heads}")
        
        # åˆ›å»ºæ¨¡å‹
        model = CustomModelForCausalLM(config)
        
        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        print(f"\næ¨¡å‹å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)")
        
        print(f"\nâœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        return True, model, config
        
    except Exception as e:
        print(f"\nâœ— æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, None, None


def test_forward_pass(model, config):
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("\n" + "="*60)
    print("æµ‹è¯• 2: å‰å‘ä¼ æ’­")
    print("="*60)
    
    try:
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        batch_size = 2
        seq_length = 10
        input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_length))
        
        print(f"è¾“å…¥ shape: {input_ids.shape}")
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = model(input_ids)
        
        logits = outputs.logits
        print(f"è¾“å‡º logits shape: {logits.shape}")
        print(f"é¢„æœŸ shape: ({batch_size}, {seq_length}, {config.vocab_size})")
        
        assert logits.shape == (batch_size, seq_length, config.vocab_size), \
            f"è¾“å‡º shape ä¸æ­£ç¡®"
        
        print(f"\nâœ“ å‰å‘ä¼ æ’­æµ‹è¯•æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"\nâœ— å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_loss_computation(model, config):
    """æµ‹è¯•æŸå¤±è®¡ç®—"""
    print("\n" + "="*60)
    print("æµ‹è¯• 3: æŸå¤±è®¡ç®—")
    print("="*60)
    
    try:
        batch_size = 2
        seq_length = 10
        input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_length))
        labels = torch.randint(0, config.vocab_size, (batch_size, seq_length))
        
        print(f"è¾“å…¥ shape: {input_ids.shape}")
        print(f"æ ‡ç­¾ shape: {labels.shape}")
        
        # å‰å‘ä¼ æ’­å¹¶è®¡ç®—æŸå¤±
        with torch.no_grad():
            outputs = model(input_ids, labels=labels)
        
        loss = outputs.loss
        print(f"æŸå¤±å€¼: {loss.item():.4f}")
        
        assert loss is not None and not torch.isnan(loss), "æŸå¤±è®¡ç®—å¼‚å¸¸"
        
        print(f"\nâœ“ æŸå¤±è®¡ç®—æµ‹è¯•æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"\nâœ— æŸå¤±è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_generation(model, config):
    """æµ‹è¯•ç”ŸæˆåŠŸèƒ½"""
    print("\n" + "="*60)
    print("æµ‹è¯• 4: æ–‡æœ¬ç”Ÿæˆ")
    print("="*60)
    
    try:
        input_ids = torch.tensor([[config.bos_token_id, 100, 200]])
        
        print(f"è¾“å…¥ token IDs: {input_ids.tolist()[0]}")
        
        # ç”Ÿæˆ
        with torch.no_grad():
            output_ids = model.generate(
                input_ids,
                max_new_tokens=5,
                do_sample=False,
                pad_token_id=config.pad_token_id,
                eos_token_id=config.eos_token_id,
            )
        
        print(f"ç”Ÿæˆçš„ token IDs: {output_ids.tolist()[0]}")
        print(f"ç”Ÿæˆäº† {output_ids.shape[1] - input_ids.shape[1]} ä¸ªæ–° token")
        
        print(f"\nâœ“ æ–‡æœ¬ç”Ÿæˆæµ‹è¯•æˆåŠŸ")
        print(f"  æ³¨æ„: ç”±äºæ¨¡å‹æœªè®­ç»ƒï¼Œç”Ÿæˆçš„ token æ˜¯éšæœºçš„")
        return True
        
    except Exception as e:
        print(f"\nâœ— æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_save_load(model, config):
    """æµ‹è¯•ä¿å­˜å’ŒåŠ è½½"""
    print("\n" + "="*60)
    print("æµ‹è¯• 5: æ¨¡å‹ä¿å­˜å’ŒåŠ è½½")
    print("="*60)
    
    import tempfile
    import os
    
    try:
        with tempfile.TemporaryDirectory() as tmpdir:
            save_dir = os.path.join(tmpdir, "test_model")
            
            # ä¿å­˜
            print(f"ä¿å­˜æ¨¡å‹åˆ°: {save_dir}")
            model.save_pretrained(save_dir)
            
            # åŠ è½½
            print(f"ä» {save_dir} åŠ è½½æ¨¡å‹")
            loaded_model = CustomModelForCausalLM.from_pretrained(save_dir)
            
            # éªŒè¯å‚æ•°ä¸€è‡´
            orig_params = sum(p.numel() for p in model.parameters())
            loaded_params = sum(p.numel() for p in loaded_model.parameters())
            
            assert orig_params == loaded_params, "å‚æ•°æ•°é‡ä¸ä¸€è‡´"
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            input_ids = torch.randint(0, config.vocab_size, (1, 5))
            with torch.no_grad():
                orig_out = model(input_ids).logits
                loaded_out = loaded_model(input_ids).logits
            
            # éªŒè¯è¾“å‡ºä¸€è‡´
            assert torch.allclose(orig_out, loaded_out, atol=1e-5), "è¾“å‡ºä¸ä¸€è‡´"
            
            print(f"\nâœ“ æ¨¡å‹ä¿å­˜å’ŒåŠ è½½æµ‹è¯•æˆåŠŸ")
            print(f"  åŸå§‹å‚æ•°é‡: {orig_params:,}")
            print(f"  åŠ è½½å‚æ•°é‡: {loaded_params:,}")
            print(f"  âœ“ éªŒè¯é€šè¿‡: å‚æ•°å’Œè¾“å‡ºä¸€è‡´")
            return True
            
    except Exception as e:
        print(f"\nâœ— ä¿å­˜å’ŒåŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_tokenizer_params_import():
    """æµ‹è¯•ä»åˆ†è¯å™¨å¯¼å…¥å‚æ•°çš„æµç¨‹ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    print("\n" + "="*60)
    print("æµ‹è¯• 6: åˆ†è¯å™¨å‚æ•°å¯¼å…¥æµç¨‹ï¼ˆæ¨¡æ‹Ÿï¼‰")
    print("="*60)
    
    try:
        # æ¨¡æ‹Ÿåˆ†è¯å™¨å‚æ•°
        class MockTokenizer:
            vocab_size = 32000
            pad_token_id = 0
            bos_token_id = 1
            eos_token_id = 2
        
        tokenizer = MockTokenizer()
        
        print("æ¨¡æ‹Ÿåˆ†è¯å™¨å‚æ•°:")
        print(f"  vocab_size: {tokenizer.vocab_size}")
        print(f"  pad_token_id: {tokenizer.pad_token_id}")
        print(f"  bos_token_id: {tokenizer.bos_token_id}")
        print(f"  eos_token_id: {tokenizer.eos_token_id}")
        
        # ä»åˆ†è¯å™¨å¯¼å…¥å‚æ•°åˆ›å»ºé…ç½®
        config = CustomModelConfig(
            vocab_size=tokenizer.vocab_size,  # ä»åˆ†è¯å™¨å¯¼å…¥
            hidden_size=768,
            num_hidden_layers=12,
            num_attention_heads=12,
            intermediate_size=3072,
            pad_token_id=tokenizer.pad_token_id,  # ä»åˆ†è¯å™¨å¯¼å…¥
            bos_token_id=tokenizer.bos_token_id,  # ä»åˆ†è¯å™¨å¯¼å…¥
            eos_token_id=tokenizer.eos_token_id,  # ä»åˆ†è¯å™¨å¯¼å…¥
        )
        
        print(f"\nåˆ›å»ºçš„æ¨¡å‹é…ç½®:")
        print(f"  vocab_size: {config.vocab_size} (ä»åˆ†è¯å™¨å¯¼å…¥)")
        print(f"  pad_token_id: {config.pad_token_id} (ä»åˆ†è¯å™¨å¯¼å…¥)")
        print(f"  bos_token_id: {config.bos_token_id} (ä»åˆ†è¯å™¨å¯¼å…¥)")
        print(f"  eos_token_id: {config.eos_token_id} (ä»åˆ†è¯å™¨å¯¼å…¥)")
        
        # éªŒè¯ä¸€è‡´æ€§
        assert config.vocab_size == tokenizer.vocab_size, "è¯æ±‡è¡¨å¤§å°ä¸ä¸€è‡´"
        assert config.pad_token_id == tokenizer.pad_token_id, "PAD token ID ä¸ä¸€è‡´"
        assert config.bos_token_id == tokenizer.bos_token_id, "BOS token ID ä¸ä¸€è‡´"
        assert config.eos_token_id == tokenizer.eos_token_id, "EOS token ID ä¸ä¸€è‡´"
        
        print(f"\nâœ“ å‚æ•°å¯¼å…¥æµç¨‹æµ‹è¯•æˆåŠŸ")
        print(f"  âœ“ æ‰€æœ‰å‚æ•°ä»åˆ†è¯å™¨æ­£ç¡®å¯¼å…¥åˆ°æ¨¡å‹é…ç½®")
        return True
        
    except Exception as e:
        print(f"\nâœ— å‚æ•°å¯¼å…¥æµç¨‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("="*60)
    print("è‡ªå®šä¹‰æ¨¡å‹åŸºç¡€åŠŸèƒ½æµ‹è¯•")
    print("="*60)
    print("\nè¿™ä¸ªæµ‹è¯•è„šæœ¬éªŒè¯:")
    print("1. æ¨¡å‹åˆ›å»º")
    print("2. å‰å‘ä¼ æ’­")
    print("3. æŸå¤±è®¡ç®—")
    print("4. æ–‡æœ¬ç”Ÿæˆ")
    print("5. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½")
    print("6. åˆ†è¯å™¨å‚æ•°å¯¼å…¥æµç¨‹ï¼ˆæ¨¡æ‹Ÿï¼‰")
    print()
    
    results = []
    
    # æµ‹è¯• 1: æ¨¡å‹åˆ›å»º
    success, model, config = test_model_creation()
    results.append(("æ¨¡å‹åˆ›å»º", success))
    
    if not success:
        print_summary(results)
        return
    
    # æµ‹è¯• 2: å‰å‘ä¼ æ’­
    success = test_forward_pass(model, config)
    results.append(("å‰å‘ä¼ æ’­", success))
    
    # æµ‹è¯• 3: æŸå¤±è®¡ç®—
    success = test_loss_computation(model, config)
    results.append(("æŸå¤±è®¡ç®—", success))
    
    # æµ‹è¯• 4: æ–‡æœ¬ç”Ÿæˆ
    success = test_generation(model, config)
    results.append(("æ–‡æœ¬ç”Ÿæˆ", success))
    
    # æµ‹è¯• 5: ä¿å­˜å’ŒåŠ è½½
    success = test_save_load(model, config)
    results.append(("æ¨¡å‹ä¿å­˜å’ŒåŠ è½½", success))
    
    # æµ‹è¯• 6: åˆ†è¯å™¨å‚æ•°å¯¼å…¥
    success = test_tokenizer_params_import()
    results.append(("åˆ†è¯å™¨å‚æ•°å¯¼å…¥æµç¨‹", success))
    
    # æ‰“å°æ€»ç»“
    print_summary(results)


def print_summary(results):
    """æ‰“å°æµ‹è¯•æ€»ç»“"""
    print("\n" + "="*60)
    print("æµ‹è¯•æ€»ç»“")
    print("="*60)
    
    for test_name, success in results:
        status = "âœ“ é€šè¿‡" if success else "âœ— å¤±è´¥"
        print(f"{status}: {test_name}")
    
    total = len(results)
    passed = sum(1 for _, success in results if success)
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å®šä¹‰æ­£ç¡®ï¼")
        print("\nä¸‹ä¸€æ­¥:")
        print("1. å®‰è£… tokenizers: pip install tokenizers")
        print("2. è®­ç»ƒåˆ†è¯å™¨: python train_tokenizer.py")
        print("3. åˆå§‹åŒ–æ¨¡å‹: python init_weights.py --tokenizer_path ./custom_tokenizer")
        print("4. è¿è¡Œå®Œæ•´æµç¨‹: bash run_full_example.sh")
    else:
        print(f"\nâš  {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    
    print("="*60)


if __name__ == '__main__':
    main()
