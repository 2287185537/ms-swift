# è‡ªå®šä¹‰é¢„è®­ç»ƒæ¨¡å‹å®Œæ•´ç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•åœ¨ ms-swift æ¡†æ¶ä¸­**ä»é›¶å¼€å§‹åˆ›å»ºå’Œé¢„è®­ç»ƒ**ä¸€ä¸ªå®Œå…¨è‡ªå®šä¹‰çš„æ¨¡å‹ã€‚

**æ ¸å¿ƒæµç¨‹ï¼š**
1. å®šä¹‰ç¬¦åˆ HuggingFace æ ‡å‡†çš„æ¨¡å‹ç»“æ„
2. åˆå§‹åŒ–éšæœºæƒé‡å¹¶ä¿å­˜
3. æ³¨å†Œæ¨¡å‹åˆ° ms-swift æ¡†æ¶
4. ä½¿ç”¨ `swift pt` å‘½ä»¤è¿›è¡Œ**å…¨é‡é¢„è®­ç»ƒ**ï¼ˆç±»ä¼¼äºé¢„è®­ç»ƒ GPT/LLaMAï¼‰
5. å¯é€‰ï¼šç»§ç»­é¢„è®­ç»ƒæˆ–è¿›è¡Œå¾®è°ƒï¼ˆSFTï¼‰

**ä¸å¾®è°ƒçš„åŒºåˆ«ï¼š**
- æœ¬ç¤ºä¾‹æ˜¯**é¢„è®­ç»ƒï¼ˆPre-trainingï¼‰**ï¼šä»éšæœºåˆå§‹åŒ–çš„æƒé‡å¼€å§‹è®­ç»ƒ
- å¾®è°ƒï¼ˆFine-tuningï¼‰ï¼šåœ¨å·²æœ‰é¢„è®­ç»ƒæƒé‡çš„åŸºç¡€ä¸Šè®­ç»ƒ
- ä½¿ç”¨ `swift pt` è€Œé `swift sft` å‘½ä»¤

## ğŸ“‹ ç›®å½•ç»“æ„

```
pretrain_example/
â”œâ”€â”€ README.md               # æœ¬æ–‡æ¡£
â”œâ”€â”€ custom_model.py         # è‡ªå®šä¹‰æ¨¡å‹å®šä¹‰ï¼ˆç¬¦åˆ HuggingFace æ ‡å‡†ï¼‰
â”œâ”€â”€ init_weights.py         # æ¨¡å‹æƒé‡åˆå§‹åŒ–è„šæœ¬
â”œâ”€â”€ register_model.py       # æ¨¡å‹æ³¨å†Œåˆ° ms-swift æ¡†æ¶
â”œâ”€â”€ pretrain.sh            # é¢„è®­ç»ƒè„šæœ¬
â”œâ”€â”€ sft.sh                 # å¾®è°ƒè„šæœ¬
â””â”€â”€ infer.sh               # æ¨ç†è„šæœ¬
```

## ğŸ“¦ ä¾èµ–å®‰è£…

åœ¨å¼€å§‹ä¹‹å‰ï¼Œç¡®ä¿å®‰è£…å¿…è¦çš„ä¾èµ–ï¼š

```bash
# å®‰è£… ms-swift (å¦‚æœè¿˜æ²¡å®‰è£…)
pip install ms-swift

# å®‰è£…åˆ†è¯å™¨è®­ç»ƒä¾èµ–
pip install tokenizers>=0.13.0

# æˆ–è€…å®‰è£…ç¤ºä¾‹ç›®å½•ä¸‹çš„æ‰€æœ‰ä¾èµ–
pip install -r requirements.txt
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ä¸€ï¼šä¸€é”®è¿è¡Œå®Œæ•´æµç¨‹ï¼ˆæ¨èï¼‰

ä½¿ç”¨ä¸€é”®è„šæœ¬å®Œæˆæ‰€æœ‰å‡†å¤‡å·¥ä½œï¼š

```bash
cd /path/to/ms-swift/examples/custom/pretrain_example

# è¿è¡Œå®Œæ•´ç¤ºä¾‹ï¼ˆåŒ…æ‹¬è®­ç»ƒåˆ†è¯å™¨ã€åˆå§‹åŒ–æ¨¡å‹ã€å‡†å¤‡æ•°æ®ï¼‰
bash run_full_example.sh
```

è¿™ä¸ªè„šæœ¬ä¼šè‡ªåŠ¨å®Œæˆï¼š
1. âœ… è®­ç»ƒè‡ªå®šä¹‰åˆ†è¯å™¨ï¼ˆBPEï¼Œ32000 è¯æ±‡ï¼‰
2. âœ… æ ¹æ®åˆ†è¯å™¨å‚æ•°åˆå§‹åŒ–æ¨¡å‹
3. âœ… å‡†å¤‡ç¤ºä¾‹é¢„è®­ç»ƒæ•°æ®
4. âœ… æµ‹è¯•æ¨¡å‹æ³¨å†Œå’Œæ¨ç†

### æ–¹å¼äºŒï¼šåˆ†æ­¥æ‰§è¡Œï¼ˆå­¦ä¹ æµç¨‹ï¼‰

#### ç¬¬ä¸€æ­¥ï¼šè®­ç»ƒè‡ªå®šä¹‰åˆ†è¯å™¨

**é‡è¦ï¼šå…ˆè®­ç»ƒåˆ†è¯å™¨ï¼Œå› ä¸ºæ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°ç­‰å‚æ•°éœ€è¦ä»åˆ†è¯å™¨å¯¼å…¥**

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒï¼ˆä¼šè‡ªåŠ¨åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼‰
python train_tokenizer.py \
    --output_dir ./custom_tokenizer \
    --vocab_size 32000

# ä½¿ç”¨è‡ªå·±çš„æ•°æ®è®­ç»ƒ
python train_tokenizer.py \
    --input_files corpus1.txt corpus2.txt \
    --output_dir ./custom_tokenizer \
    --vocab_size 32000 \
    --min_frequency 2
```

**å‚æ•°è¯´æ˜ï¼š**
- `--input_files`: è®­ç»ƒæ•°æ®æ–‡ä»¶ï¼ˆçº¯æ–‡æœ¬ï¼Œæ¯è¡Œä¸€ä¸ªæ ·æœ¬ï¼‰
- `--output_dir`: åˆ†è¯å™¨ä¿å­˜è·¯å¾„
- `--vocab_size`: è¯æ±‡è¡¨å¤§å°ï¼ˆæ¨è 32000/50000ï¼‰
- `--min_frequency`: æœ€å°è¯é¢‘é˜ˆå€¼

#### ç¬¬äºŒæ­¥ï¼šæ ¹æ®åˆ†è¯å™¨å‚æ•°åˆå§‹åŒ–æ¨¡å‹

**å…³é”®æµç¨‹ï¼šä»åˆ†è¯å™¨å¯¼å…¥è¯æ±‡è¡¨å¤§å°ç­‰å‚æ•°åˆ°æ¨¡å‹é…ç½®**

```bash
# ä½¿ç”¨å·²è®­ç»ƒçš„åˆ†è¯å™¨åˆå§‹åŒ–æ¨¡å‹
python init_weights.py \
    --tokenizer_path ./custom_tokenizer \
    --output_dir ./custom_pretrained_model \
    --hidden_size 768

# æˆ–è€…åŒæ—¶è®­ç»ƒæ–°åˆ†è¯å™¨å¹¶åˆå§‹åŒ–æ¨¡å‹
python init_weights.py \
    --train_tokenizer \
    --output_dir ./custom_pretrained_model \
    --hidden_size 768
```

è¿™ä¸ªè„šæœ¬çš„æµç¨‹ï¼š
1. åŠ è½½æˆ–è®­ç»ƒè‡ªå®šä¹‰åˆ†è¯å™¨
2. **ä»åˆ†è¯å™¨è·å–è¯æ±‡è¡¨å¤§å°ã€ç‰¹æ®Š token ID ç­‰å‚æ•°**
3. **ä½¿ç”¨è¿™äº›å‚æ•°åˆ›å»ºæ¨¡å‹é…ç½®**
4. åˆå§‹åŒ–æ¨¡å‹éšæœºæƒé‡
5. ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨
6. éªŒè¯æ¨¡å‹å’Œåˆ†è¯å™¨çš„ä¸€è‡´æ€§

**å‚æ•°è¯´æ˜ï¼š**
- `--tokenizer_path`: å·²æœ‰åˆ†è¯å™¨è·¯å¾„ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œä¼šè®­ç»ƒæ–°åˆ†è¯å™¨ï¼‰
- `--train_tokenizer`: å¼ºåˆ¶è®­ç»ƒæ–°åˆ†è¯å™¨
- `--output_dir`: æ¨¡å‹ä¿å­˜è·¯å¾„
- `--hidden_size`: éšè—å±‚ç»´åº¦ï¼ˆé»˜è®¤ 768ï¼Œå¯é€‰ 768/1024/2048ï¼‰

**è¾“å‡ºæ–‡ä»¶ï¼š**
```
custom_pretrained_model/
â”œâ”€â”€ config.json              # æ¨¡å‹é…ç½®ï¼ˆåŒ…å«ä»åˆ†è¯å™¨å¯¼å…¥çš„å‚æ•°ï¼‰
â”œâ”€â”€ pytorch_model.bin        # æ¨¡å‹æƒé‡ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰
â”œâ”€â”€ tokenizer_config.json    # åˆ†è¯å™¨é…ç½®
â”œâ”€â”€ tokenizer.json           # åˆ†è¯å™¨è¯æ±‡è¡¨
â””â”€â”€ special_tokens_map.json  # ç‰¹æ®Š token æ˜ å°„
```

#### ç¬¬ä¸‰æ­¥ï¼šæµ‹è¯•æ¨¡å‹æ³¨å†Œ

éªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®æ³¨å†Œåˆ° ms-swift æ¡†æ¶ï¼š

```bash
python register_model.py
```

å¦‚æœçœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹è¾“å‡ºï¼Œè¯´æ˜æ³¨å†ŒæˆåŠŸï¼š

```
âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ!
  - è¯æ±‡è¡¨å¤§å°: 32000
âœ“ æ¨ç†æˆåŠŸ!
```

### ç¬¬ä¸‰æ­¥ï¼šå‡†å¤‡é¢„è®­ç»ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰

å¦‚æœä½ æœ‰è‡ªå·±çš„æ–‡æœ¬è¯­æ–™ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·è„šæœ¬å‡†å¤‡æ•°æ®ï¼š

```bash
# åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
python prepare_pretrain_data.py sample \
    --output ./data/sample_pretrain.jsonl \
    --num_samples 1000

# ä»æ–‡æœ¬æ–‡ä»¶è½¬æ¢
python prepare_pretrain_data.py convert \
    --input corpus1.txt corpus2.txt \
    --output ./data/train.jsonl

# ä»ç›®å½•æ‰¹é‡è½¬æ¢
python prepare_pretrain_data.py corpus \
    --input_dir ./corpus \
    --output ./data/train.jsonl

# éªŒè¯æ•°æ®æ ¼å¼
python prepare_pretrain_data.py validate \
    --input ./data/train.jsonl
```

### ç¬¬å››æ­¥ï¼šå…¨é‡é¢„è®­ç»ƒæ¨¡å‹

ä½¿ç”¨å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®ä»é›¶å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼š

```bash
bash pretrain.sh
```

**é¢„è®­ç»ƒè„šæœ¬è¯´æ˜ï¼š**
- ä½¿ç”¨ `swift pt` å‘½ä»¤è¿›è¡Œ**å…¨é‡é¢„è®­ç»ƒ**ï¼ˆPre-trainingï¼‰
- ä»éšæœºåˆå§‹åŒ–çš„æƒé‡å¼€å§‹è®­ç»ƒ
- é€‚åˆå¤§è§„æ¨¡æ— æ ‡æ³¨æ–‡æœ¬æ•°æ®
- æ”¯æŒå¤š GPU è®­ç»ƒï¼ˆè®¾ç½® `CUDA_VISIBLE_DEVICES`ï¼‰
- æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
- æ”¯æŒ BF16 æ··åˆç²¾åº¦è®­ç»ƒ

**é¢„è®­ç»ƒ vs å¾®è°ƒï¼š**
- **é¢„è®­ç»ƒï¼ˆptï¼‰**ï¼šä»é›¶å¼€å§‹è®­ç»ƒï¼Œå­¦ä¹ è¯­è¨€çš„åŸºç¡€çŸ¥è¯†
- **å¾®è°ƒï¼ˆsftï¼‰**ï¼šåœ¨å·²æœ‰æ¨¡å‹åŸºç¡€ä¸Šè®­ç»ƒï¼Œå­¦ä¹ ç‰¹å®šä»»åŠ¡

**å¯ä»¥ä¿®æ”¹çš„å‚æ•°ï¼š**
- `MODEL_PATH`: ä½ åˆå§‹åŒ–çš„éšæœºæƒé‡æ¨¡å‹è·¯å¾„
- `DATASET`: é¢„è®­ç»ƒæ•°æ®é›†ï¼ˆæ¨èä½¿ç”¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ï¼‰
  - ç¤ºä¾‹ï¼š`AI-ModelScope/wikipedia-cn-20230720-filtered`
  - æˆ–æœ¬åœ°æ–‡æœ¬æ–‡ä»¶ï¼š`--dataset train.jsonl`
- `OUTPUT_DIR`: è¾“å‡ºè·¯å¾„
- è®­ç»ƒè¶…å‚æ•°ï¼šå­¦ä¹ ç‡ã€batch sizeã€è®­ç»ƒè½®æ•°ç­‰

**é¢„è®­ç»ƒæ•°æ®æ ¼å¼ï¼š**
```jsonl
{"text": "è¿™æ˜¯ç¬¬ä¸€æ®µé¢„è®­ç»ƒæ–‡æœ¬..."}
{"text": "è¿™æ˜¯ç¬¬äºŒæ®µé¢„è®­ç»ƒæ–‡æœ¬..."}
```

æˆ–çº¯æ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªæ ·æœ¬ï¼‰ï¼š
```
è¿™æ˜¯ç¬¬ä¸€æ®µé¢„è®­ç»ƒæ–‡æœ¬...
è¿™æ˜¯ç¬¬äºŒæ®µé¢„è®­ç»ƒæ–‡æœ¬...
```

### ç¬¬äº”æ­¥ï¼šå¾®è°ƒæ¨¡å‹ï¼ˆå¯é€‰ï¼‰

åœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šè¿›è¡Œ SFT å¾®è°ƒï¼š

```bash
bash sft.sh
```

**å¾®è°ƒè„šæœ¬è¯´æ˜ï¼š**
- ä½¿ç”¨ LoRA è¿›è¡Œé«˜æ•ˆå¾®è°ƒ
- å¯ä»¥åˆ‡æ¢ä¸ºå…¨å‚æ•°å¾®è°ƒï¼ˆ`--train_type full`ï¼‰
- æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†

### ç¬¬å…­æ­¥ï¼šæ¨ç†æµ‹è¯•

ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†ï¼š

```bash
bash infer.sh
```

è¿›å…¥äº¤äº’å¼å¯¹è¯ï¼š
```
<<< ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±
>>> ï¼ˆæ¨¡å‹å›å¤ï¼‰

<<< è®²ä¸ªç¬‘è¯
>>> ï¼ˆæ¨¡å‹å›å¤ï¼‰
```

## ğŸ“š è¯¦ç»†è¯´æ˜

### 1. è‡ªå®šä¹‰æ¨¡å‹ç»“æ„ (`custom_model.py`)

è¿™ä¸ªæ–‡ä»¶å®šä¹‰äº†ä¸€ä¸ªå®Œæ•´çš„ Transformer è§£ç å™¨æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š

**ä¸»è¦ç»„ä»¶ï¼š**
- `CustomModelConfig`: æ¨¡å‹é…ç½®ç±»ï¼ˆç»§æ‰¿è‡ª `PretrainedConfig`ï¼‰
- `CustomAttention`: å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚
- `CustomMLP`: å‰é¦ˆç¥ç»ç½‘ç»œå±‚
- `CustomTransformerLayer`: Transformer è§£ç å™¨å±‚
- `CustomModel`: æ¨¡å‹ä¸»ä½“
- `CustomModelForCausalLM`: å› æœè¯­è¨€å»ºæ¨¡æ¨¡å‹ï¼ˆå¸¦ LM Headï¼‰

**å…³é”®ç‰¹æ€§ï¼š**
- âœ… ç¬¦åˆ HuggingFace æ ‡å‡†æ¥å£
- âœ… æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
- âœ… æ”¯æŒ KV Cacheï¼ˆåŠ é€Ÿæ¨ç†ï¼‰
- âœ… æ”¯æŒ Flash Attentionï¼ˆéœ€è¦å®‰è£…ï¼‰
- âœ… å®Œæ•´çš„ä¸­æ–‡æ³¨é‡Š

**æ¨¡å‹æ¶æ„ï¼š**
```
è¾“å…¥ tokens
    â†“
Token Embedding + Position Embedding
    â†“
Transformer Layer Ã— N
    â”œâ”€â”€ Layer Norm
    â”œâ”€â”€ Multi-Head Attention
    â”œâ”€â”€ Residual Connection
    â”œâ”€â”€ Layer Norm
    â”œâ”€â”€ Feed Forward Network
    â””â”€â”€ Residual Connection
    â†“
Final Layer Norm
    â†“
LM Head (è¾“å‡º logits)
```

### 2. åˆ†è¯å™¨è®­ç»ƒ (`train_tokenizer.py`)

è¿™ä¸ªè„šæœ¬è´Ÿè´£è®­ç»ƒ BPE åˆ†è¯å™¨ï¼š

**è®­ç»ƒæµç¨‹ï¼š**
1. åˆ›å»º BPE (Byte Pair Encoding) æ¨¡å‹
2. åœ¨æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒï¼Œå­¦ä¹ æœ€ä¼˜çš„ subword åˆ‡åˆ†
3. ä¿å­˜ä¸º HuggingFace æ ¼å¼
4. è‡ªåŠ¨æµ‹è¯•åˆ†è¯æ•ˆæœ

**å…³é”®ç‰¹æ€§ï¼š**
- æ”¯æŒä»»æ„ Unicode å­—ç¬¦
- å¯è‡ªå®šä¹‰ç‰¹æ®Š token
- è‡ªåŠ¨ç”Ÿæˆè¯æ±‡è¡¨
- æä¾›ç¼–ç /è§£ç æµ‹è¯•

**è¾“å‡ºï¼š**
```
custom_tokenizer/
â”œâ”€â”€ tokenizer_config.json    # åˆ†è¯å™¨é…ç½®
â”œâ”€â”€ tokenizer.json           # è¯æ±‡è¡¨å’Œåˆå¹¶è§„åˆ™
â””â”€â”€ special_tokens_map.json  # ç‰¹æ®Š token æ˜ å°„
```

### 3. æƒé‡åˆå§‹åŒ– (`init_weights.py`)

**æ ¸å¿ƒæµç¨‹ï¼šåˆ†è¯å™¨å‚æ•° â†’ æ¨¡å‹é…ç½®**

è¿™ä¸ªè„šæœ¬çš„å…³é”®åˆ›æ–°æ˜¯ï¼š**å…ˆå‡†å¤‡åˆ†è¯å™¨ï¼Œå†æ ¹æ®åˆ†è¯å™¨å‚æ•°åˆå§‹åŒ–æ¨¡å‹**

```python
# ä¼ªä»£ç è¯´æ˜å‚æ•°å¯¼å…¥æµç¨‹
tokenizer = load_or_train_tokenizer()  # æ­¥éª¤1: å‡†å¤‡åˆ†è¯å™¨

# æ­¥éª¤2: ä»åˆ†è¯å™¨å¯¼å…¥å‚æ•°
vocab_size = tokenizer.vocab_size          # è¯æ±‡è¡¨å¤§å°
pad_token_id = tokenizer.pad_token_id      # PAD token ID
bos_token_id = tokenizer.bos_token_id      # BOS token ID
eos_token_id = tokenizer.eos_token_id      # EOS token ID

# æ­¥éª¤3: ä½¿ç”¨è¿™äº›å‚æ•°åˆ›å»ºæ¨¡å‹é…ç½®
config = CustomModelConfig(
    vocab_size=vocab_size,        # ä»åˆ†è¯å™¨å¯¼å…¥ï¼
    pad_token_id=pad_token_id,    # ä»åˆ†è¯å™¨å¯¼å…¥ï¼
    bos_token_id=bos_token_id,    # ä»åˆ†è¯å™¨å¯¼å…¥ï¼
    eos_token_id=eos_token_id,    # ä»åˆ†è¯å™¨å¯¼å…¥ï¼
    hidden_size=768,              # ç”¨æˆ·æŒ‡å®š
    num_hidden_layers=12,         # ç”¨æˆ·æŒ‡å®š
    ...
)

# æ­¥éª¤4: åˆå§‹åŒ–æ¨¡å‹
model = CustomModelForCausalLM(config)

# æ­¥éª¤5: éªŒè¯ä¸€è‡´æ€§
assert model.config.vocab_size == tokenizer.vocab_size
```

**ä¸ºä»€ä¹ˆè¦è¿™æ ·åšï¼Ÿ**
1. âœ… **ä¿è¯ä¸€è‡´æ€§**ï¼šæ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°å¿…é¡»ä¸åˆ†è¯å™¨å®Œå…¨ä¸€è‡´
2. âœ… **é¿å…é”™è¯¯**ï¼šæ‰‹åŠ¨æŒ‡å®šå®¹æ˜“å‡ºé”™ï¼Œä»åˆ†è¯å™¨å¯¼å…¥æ›´å¯é 
3. âœ… **çµæ´»æ€§**ï¼šå¯ä»¥æ–¹ä¾¿åœ°ä½¿ç”¨ä¸åŒè¯æ±‡è¡¨å¤§å°çš„åˆ†è¯å™¨
4. âœ… **ç¬¦åˆæœ€ä½³å®è·µ**ï¼šè¿™æ˜¯å·¥ä¸šç•Œæ ‡å‡†æµç¨‹

**åˆå§‹åŒ–ç­–ç•¥ï¼š**
- ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–æƒé‡ï¼ˆæ ‡å‡†å·® 0.02ï¼‰
- Linear å±‚çš„ bias åˆå§‹åŒ–ä¸º 0
- Embedding å±‚ä½¿ç”¨æ­£æ€åˆ†å¸ƒ
- LayerNorm çš„ weight åˆå§‹åŒ–ä¸º 1ï¼Œbias åˆå§‹åŒ–ä¸º 0

**è¾“å‡ºæ–‡ä»¶ï¼š**
```
custom_pretrained_model/
â”œâ”€â”€ config.json              # æ¨¡å‹é…ç½®ï¼ˆåŒ…å«ä»åˆ†è¯å™¨å¯¼å…¥çš„å‚æ•°ï¼‰
â”œâ”€â”€ pytorch_model.bin        # æ¨¡å‹æƒé‡ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰
â”œâ”€â”€ tokenizer_config.json    # åˆ†è¯å™¨é…ç½®ï¼ˆä»åˆ†è¯å™¨å¤åˆ¶ï¼‰
â”œâ”€â”€ tokenizer.json           # åˆ†è¯å™¨è¯æ±‡è¡¨ï¼ˆä»åˆ†è¯å™¨å¤åˆ¶ï¼‰
â””â”€â”€ special_tokens_map.json  # ç‰¹æ®Š token æ˜ å°„ï¼ˆä»åˆ†è¯å™¨å¤åˆ¶ï¼‰
```

### 3. æ¨¡å‹æ³¨å†Œ (`register_model.py`)

è¿™ä¸ªæ–‡ä»¶å°†è‡ªå®šä¹‰æ¨¡å‹æ³¨å†Œåˆ° ms-swift æ¡†æ¶ï¼š

**æ³¨å†Œå†…å®¹ï¼š**
1. **æ¨¡å‹åŠ è½½å‡½æ•°** (`get_custom_model_tokenizer`)
   - è´Ÿè´£åŠ è½½æ¨¡å‹å’Œ tokenizer
   - æ”¯æŒè‡ªå®šä¹‰åŠ è½½é€»è¾‘
   - å¯ä»¥æ·»åŠ é¢„å¤„ç†/åå¤„ç†

2. **å¯¹è¯æ¨¡æ¿** (`register_template`)
   - å®šä¹‰ç”¨æˆ·å’ŒåŠ©æ‰‹çš„å¯¹è¯æ ¼å¼
   - è®¾ç½®ç‰¹æ®Š tokenï¼ˆå¦‚ `<|endoftext|>`ï¼‰
   - å¯ä»¥è‡ªå®šä¹‰ system prompt

3. **æ¨¡å‹å…ƒä¿¡æ¯** (`register_model`)
   - æ¨¡å‹ç±»å‹æ ‡è¯†
   - æ”¯æŒçš„æ¨¡å‹è·¯å¾„
   - ä¾èµ–åŒ…åˆ—è¡¨
   - æ¨¡å‹æ ‡ç­¾

**å¯¹è¯æ¨¡æ¿ç¤ºä¾‹ï¼š**
```
<|system|>
You are a helpful assistant.
<|user|>
ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±
<|assistant|>
ï¼ˆæ¨¡å‹å›å¤ï¼‰
<|endoftext|>
```

### 4. è®­ç»ƒè„šæœ¬

#### é¢„è®­ç»ƒ (`pretrain.sh`)

**swift pt å‘½ä»¤è¯´æ˜ï¼ˆPre-trainingï¼Œå…¨é‡é¢„è®­ç»ƒï¼‰ï¼š**
```bash
swift pt \
    --custom_register_path register_model.py \  # è‡ªå®šä¹‰æ¨¡å‹æ³¨å†Œæ–‡ä»¶
    --model_type custom_pretrain \              # æ¨¡å‹ç±»å‹
    --model ./custom_pretrained_model \         # éšæœºåˆå§‹åŒ–çš„æ¨¡å‹è·¯å¾„
    --dataset <dataset_name> \                  # é¢„è®­ç»ƒæ•°æ®é›†
    --train_type full \                         # å…¨å‚æ•°è®­ç»ƒï¼ˆé¢„è®­ç»ƒå¿…é¡»ç”¨ fullï¼‰
    --num_train_epochs 1 \                      # è®­ç»ƒè½®æ•°
    --per_device_train_batch_size 4 \           # æ¯ä¸ªè®¾å¤‡çš„ batch size
    --learning_rate 1e-4 \                      # å­¦ä¹ ç‡ï¼ˆé¢„è®­ç»ƒé€šå¸¸ç”¨è¾ƒå¤§å­¦ä¹ ç‡ï¼‰
    --weight_decay 0.1 \                        # æƒé‡è¡°å‡ï¼ˆé¢„è®­ç»ƒæ¨è 0.1ï¼‰
    --warmup_ratio 0.01 \                       # é¢„çƒ­æ¯”ä¾‹ï¼ˆé¢„è®­ç»ƒæ¨è 0.01-0.03ï¼‰
    --gradient_accumulation_steps 4 \           # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    --max_length 2048 \                         # æœ€å¤§åºåˆ—é•¿åº¦
    --output_dir ./output/custom_pretrain \     # è¾“å‡ºè·¯å¾„
    --bf16 true \                               # ä½¿ç”¨ BF16 æ··åˆç²¾åº¦
    --use_flash_attn true                       # ä½¿ç”¨ Flash Attention åŠ é€Ÿ
```

**é¢„è®­ç»ƒæ•°æ®æ ¼å¼ï¼ˆçº¯æ–‡æœ¬è¯­æ–™ï¼‰ï¼š**

1. **JSONL æ ¼å¼**ï¼ˆæ¨èï¼‰ï¼š
```jsonl
{"text": "è¿™æ˜¯ä¸€æ®µé¢„è®­ç»ƒæ–‡æœ¬ï¼Œå¯ä»¥æ˜¯ç»´åŸºç™¾ç§‘ã€ä¹¦ç±ã€ç½‘é¡µç­‰ä»»æ„æ–‡æœ¬..."}
{"text": "å¦ä¸€æ®µé¢„è®­ç»ƒæ–‡æœ¬..."}
```

2. **çº¯æ–‡æœ¬æ ¼å¼**ï¼š
```
è¿™æ˜¯ç¬¬ä¸€æ®µé¢„è®­ç»ƒæ–‡æœ¬ã€‚
è¿™æ˜¯ç¬¬äºŒæ®µé¢„è®­ç»ƒæ–‡æœ¬ã€‚
```

3. **é¢„è®­ç»ƒæ•°æ®é›†æ¨è**ï¼š
   - ä¸­æ–‡ï¼š`AI-ModelScope/wikipedia-cn-20230720-filtered`
   - è‹±æ–‡ï¼š`c4`ï¼Œ`pile`
   - ä»£ç ï¼š`bigcode/the-stack`
   - å¤šè¯­è¨€ï¼š`mc4`

**æ³¨æ„äº‹é¡¹ï¼š**
- é¢„è®­ç»ƒ**å¿…é¡»ä½¿ç”¨ `--train_type full`**ï¼ˆå…¨å‚æ•°è®­ç»ƒï¼‰
- é¢„è®­ç»ƒæ•°æ®åº”è¯¥æ˜¯**å¤§è§„æ¨¡æ— æ ‡æ³¨æ–‡æœ¬**
- é¢„è®­ç»ƒéœ€è¦è¾ƒé•¿æ—¶é—´å’Œå¤§é‡æ•°æ®ï¼ˆé€šå¸¸éœ€è¦æ•°å GB åˆ°æ•°ç™¾ GB æ–‡æœ¬ï¼‰
- å­¦ä¹ ç‡é€šå¸¸æ¯”å¾®è°ƒæ›´å¤§ï¼ˆ1e-4 vs 1e-5ï¼‰
- æƒé‡è¡°å‡é€šå¸¸æ›´å¤§ï¼ˆ0.1 vs 0.01ï¼‰

#### å¾®è°ƒ (`sft.sh`)

ä¸é¢„è®­ç»ƒç±»ä¼¼ï¼Œä½†ä½¿ç”¨ `swift sft` å‘½ä»¤ï¼Œå¹¶ä¸”ï¼š
- é»˜è®¤ä½¿ç”¨ LoRA è®­ç»ƒï¼ˆæ›´é«˜æ•ˆï¼‰
- å¯ä»¥åœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒ
- æ”¯æŒæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†

#### æ¨ç† (`infer.sh`)

**æ¨ç†æ¨¡å¼ï¼š**
1. **äº¤äº’å¼æ¨ç†**ï¼ˆé»˜è®¤ï¼‰
   - å¯åŠ¨åè¿›å…¥å¯¹è¯æ¨¡å¼
   - è¾“å…¥é—®é¢˜ï¼Œæ¨¡å‹ç”Ÿæˆå›å¤

2. **æ‰¹é‡æ¨ç†**
   ```bash
   swift infer \
       --custom_register_path register_model.py \
       --model ./custom_pretrained_model \
       --val_dataset <dataset> \
       --infer_backend pt
   ```

3. **ä½¿ç”¨å¾®è°ƒåçš„ adapter**
   ```bash
   swift infer \
       --adapters ./output/custom_sft/checkpoint-xxx \
       --load_data_args true
   ```

## ğŸ”§ é«˜çº§ç”¨æ³•

### 1. è‡ªå®šä¹‰æ¨¡å‹ç»“æ„

å¦‚æœéœ€è¦ä¿®æ”¹æ¨¡å‹ç»“æ„ï¼Œç¼–è¾‘ `custom_model.py`ï¼š

**å¢åŠ å±‚æ•°ï¼š**
```python
config = CustomModelConfig(
    num_hidden_layers=24,  # ä» 12 æ”¹ä¸º 24
    ...
)
```

**ä¿®æ”¹æ³¨æ„åŠ›å¤´æ•°ï¼š**
```python
config = CustomModelConfig(
    num_attention_heads=16,  # ä» 12 æ”¹ä¸º 16
    hidden_size=1024,        # å¿…é¡»èƒ½è¢« num_attention_heads æ•´é™¤
    ...
)
```

**æ·»åŠ  RoPE (Rotary Position Embedding)ï¼š**
å¯ä»¥å‚è€ƒ LLaMA çš„å®ç°ï¼Œåœ¨ `CustomAttention` ä¸­æ·»åŠ  RoPEã€‚

### 2. ä½¿ç”¨è‡ªå·±çš„ Tokenizer

å¦‚æœéœ€è¦è®­ç»ƒè‡ªå·±çš„ tokenizerï¼š

```python
from tokenizers import Tokenizer, models, trainers, pre_tokenizers

# è®­ç»ƒ BPE tokenizer
tokenizer = Tokenizer(models.BPE())
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()

trainer = trainers.BpeTrainer(
    vocab_size=32000,
    special_tokens=["<|endoftext|>", "<|user|>", "<|assistant|>"]
)

# åœ¨ä½ çš„æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒ
tokenizer.train(files=["your_data.txt"], trainer=trainer)

# ä¿å­˜
tokenizer.save("custom_tokenizer.json")
```

ç„¶ååœ¨ `init_weights.py` ä¸­åŠ è½½ä½ çš„ tokenizerã€‚

### 3. ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†

åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†ï¼ˆå‚è€ƒ `examples/custom/dataset.py`ï¼‰ï¼š

```python
from swift.llm import DatasetMeta, ResponsePreprocessor, register_dataset

class MyPreprocessor(ResponsePreprocessor):
    def preprocess(self, row):
        return {
            'messages': [
                {'role': 'user', 'content': row['input']},
                {'role': 'assistant', 'content': row['output']}
            ]
        }

register_dataset(
    DatasetMeta(
        ms_dataset_id='my_dataset',
        preprocess_func=MyPreprocessor(),
    ))
```

ç„¶ååœ¨è®­ç»ƒæ—¶ï¼š
```bash
swift pt \
    --custom_register_path register_model.py \
                           my_dataset.py \
    --dataset my_dataset \
    ...
```

### 4. å¤š GPU è®­ç»ƒ

**æ•°æ®å¹¶è¡Œï¼š**
```bash
CUDA_VISIBLE_DEVICES=0,1,2,3 \
swift pt \
    --model ./custom_pretrained_model \
    --dataset <dataset> \
    ...
```

**DeepSpeed ZeRO-2ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰ï¼š**
```bash
swift pt \
    --model ./custom_pretrained_model \
    --dataset <dataset> \
    --deepspeed default-zero2 \
    ...
```

**DeepSpeed ZeRO-3ï¼ˆæè‡´èŠ‚çœæ˜¾å­˜ï¼‰ï¼š**
```bash
swift pt \
    --model ./custom_pretrained_model \
    --dataset <dataset> \
    --deepspeed default-zero3 \
    ...
```

### 5. æ¨¡å‹é‡åŒ–

**ä½¿ç”¨ GPTQ é‡åŒ–ï¼š**
```bash
swift export \
    --model ./output/custom_pretrain/checkpoint-xxx \
    --quant_bits 4 \
    --quant_method gptq \
    --output_dir ./quantized_model
```

**ä½¿ç”¨ AWQ é‡åŒ–ï¼š**
```bash
swift export \
    --model ./output/custom_pretrain/checkpoint-xxx \
    --quant_bits 4 \
    --quant_method awq \
    --output_dir ./quantized_model
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### æ˜¾å­˜ä¼˜åŒ–

1. **ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹**
   ```bash
   --gradient_checkpointing true
   ```
   - ç”¨æ—¶é—´æ¢æ˜¾å­˜
   - å¯ä»¥èŠ‚çœ 30-50% æ˜¾å­˜
   - è®­ç»ƒé€Ÿåº¦é™ä½ 20-30%

2. **ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ**
   ```bash
   --bf16 true  # æˆ– --fp16 true
   ```
   - èŠ‚çœ 50% æ˜¾å­˜
   - åŠ é€Ÿè®­ç»ƒ 2-3 å€
   - æ•°å€¼ç¨³å®šæ€§å¥½ï¼ˆBF16 ä¼˜äº FP16ï¼‰

3. **å‡å° batch sizeï¼Œå¢åŠ æ¢¯åº¦ç´¯ç§¯**
   ```bash
   --per_device_train_batch_size 1 \
   --gradient_accumulation_steps 16
   ```
   - ç­‰æ•ˆ batch size = 1 Ã— 16 = 16
   - æ˜¾å­˜å ç”¨æ›´å°‘

4. **ä½¿ç”¨ LoRA è€Œéå…¨å‚æ•°è®­ç»ƒ**
   ```bash
   --train_type lora \
   --lora_rank 8
   ```
   - åªè®­ç»ƒ < 1% çš„å‚æ•°
   - æ˜¾å­˜å ç”¨å¤§å¹…é™ä½

### è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–

1. **ä½¿ç”¨ Flash Attention**
   ```bash
   pip install flash-attn
   ```
   - è‡ªåŠ¨å¯ç”¨ï¼ˆå¦‚æœå®‰è£…ï¼‰
   - åŠ é€Ÿæ³¨æ„åŠ›è®¡ç®— 2-4 å€

2. **å¢åŠ æ•°æ®åŠ è½½ workers**
   ```bash
   --dataloader_num_workers 8 \
   --dataset_num_proc 16
   ```

3. **ä½¿ç”¨æ›´å¤§çš„ batch size**
   ```bash
   --per_device_train_batch_size 4 \
   --gradient_accumulation_steps 4
   ```

## ğŸ› å¸¸è§é—®é¢˜

### 1. OOM (Out of Memory) é”™è¯¯

**è§£å†³æ–¹æ¡ˆï¼š**
- å‡å° `per_device_train_batch_size`
- å¯ç”¨ `gradient_checkpointing`
- ä½¿ç”¨ DeepSpeed ZeRO
- å‡å° `max_length`

### 2. è®­ç»ƒé€Ÿåº¦å¾ˆæ…¢

**è§£å†³æ–¹æ¡ˆï¼š**
- å®‰è£… Flash Attention
- ä½¿ç”¨ BF16/FP16 æ··åˆç²¾åº¦
- å¢åŠ  `dataloader_num_workers`
- æ£€æŸ¥æ˜¯å¦åœ¨ä½¿ç”¨ CPUï¼ˆåº”è¯¥ä½¿ç”¨ GPUï¼‰

### 3. æ¨¡å‹åŠ è½½å¤±è´¥

**æ£€æŸ¥ï¼š**
- æ˜¯å¦è¿è¡Œäº† `init_weights.py`
- `register_model.py` ä¸­çš„æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®
- æ˜¯å¦å¯¼å…¥äº† `custom_model.py`

### 4. ç”Ÿæˆè´¨é‡ä¸å¥½

**å¯èƒ½åŸå› ï¼š**
- è®­ç»ƒæ•°æ®ä¸è¶³
- è®­ç»ƒè½®æ•°å¤ªå°‘
- å­¦ä¹ ç‡ä¸åˆé€‚
- éœ€è¦æ›´å¤šçš„è®­ç»ƒæ•°æ®å’Œæ›´é•¿çš„è®­ç»ƒæ—¶é—´

## ğŸ“– å‚è€ƒèµ„æ–™

- [ms-swift å®˜æ–¹æ–‡æ¡£](https://github.com/modelscope/ms-swift)
- [HuggingFace Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers)
- [æ·±åº¦å­¦ä¹ ä¼˜åŒ–æŠ€å·§](https://github.com/modelscope/ms-swift/blob/main/docs)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯è¯

æœ¬ç¤ºä¾‹éµå¾ª Apache 2.0 è®¸å¯è¯ã€‚
