# å¿«é€Ÿå¼€å§‹æŒ‡å—

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„è‡ªå®šä¹‰é¢„è®­ç»ƒæ¨¡å‹ç¤ºä¾‹ï¼Œå±•ç¤ºäº†**ä»è®­ç»ƒåˆ†è¯å™¨åˆ°é¢„è®­ç»ƒæ¨¡å‹**çš„å®Œæ•´æµç¨‹ã€‚

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

æœ¬ç¤ºä¾‹çš„æ ¸å¿ƒåˆ›æ–°æ˜¯ï¼š**åˆ†è¯å™¨ä¼˜å…ˆï¼Œå‚æ•°å¯¼å…¥**

ä¼ ç»Ÿæµç¨‹ï¼ˆå®¹æ˜“å‡ºé”™ï¼‰ï¼š
```
âŒ æ‰‹åŠ¨æŒ‡å®š vocab_size â†’ åˆ›å»ºæ¨¡å‹ â†’ è®­ç»ƒåˆ†è¯å™¨ â†’ å¯èƒ½ä¸ä¸€è‡´
```

æœ¬ç¤ºä¾‹æµç¨‹ï¼ˆæœ€ä½³å®è·µï¼‰ï¼š
```
âœ… è®­ç»ƒåˆ†è¯å™¨ â†’ ä»åˆ†è¯å™¨å¯¼å…¥å‚æ•° â†’ åˆ›å»ºæ¨¡å‹ â†’ ç¡®ä¿ä¸€è‡´æ€§
```

## ğŸ“¦ å®‰è£…ä¾èµ–

```bash
# 1. å®‰è£… ms-swift
pip install ms-swift

# 2. å®‰è£…åˆ†è¯å™¨è®­ç»ƒå·¥å…·
pip install tokenizers>=0.13.0

# 3. æˆ–è€…ä¸€æ¬¡æ€§å®‰è£…æ‰€æœ‰ä¾èµ–
pip install -r requirements.txt
```

## ğŸš€ ä¸‰ç§ä½¿ç”¨æ–¹å¼

### æ–¹å¼ä¸€ï¼šä¸€é”®è¿è¡Œï¼ˆæœ€å¿«ï¼‰

```bash
bash run_full_example.sh
```

è¿™ä¸ªè„šæœ¬ä¼šè‡ªåŠ¨ï¼š
- âœ… è®­ç»ƒä¸€ä¸ª 32000 è¯æ±‡çš„ BPE åˆ†è¯å™¨
- âœ… æ ¹æ®åˆ†è¯å™¨å‚æ•°åˆå§‹åŒ–æ¨¡å‹
- âœ… å‡†å¤‡ç¤ºä¾‹é¢„è®­ç»ƒæ•°æ®
- âœ… æµ‹è¯•æ¨¡å‹æ³¨å†Œå’Œæ¨ç†

### æ–¹å¼äºŒï¼šåˆ†æ­¥æ‰§è¡Œï¼ˆå­¦ä¹ æµç¨‹ï¼‰

#### æ­¥éª¤ 1: è®­ç»ƒåˆ†è¯å™¨

```bash
# ä½¿ç”¨ç¤ºä¾‹æ•°æ®è®­ç»ƒï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
python train_tokenizer.py \
    --output_dir ./custom_tokenizer \
    --vocab_size 32000

# ä½¿ç”¨è‡ªå·±çš„æ•°æ®è®­ç»ƒ
python train_tokenizer.py \
    --input_files corpus1.txt corpus2.txt corpus3.txt \
    --output_dir ./custom_tokenizer \
    --vocab_size 32000
```

#### æ­¥éª¤ 2: åˆå§‹åŒ–æ¨¡å‹ï¼ˆä»åˆ†è¯å™¨å¯¼å…¥å‚æ•°ï¼‰

```bash
python init_weights.py \
    --tokenizer_path ./custom_tokenizer \
    --output_dir ./custom_pretrained_model \
    --hidden_size 768
```

**å…³é”®ï¼šè¿™ä¸€æ­¥ä¼šä»åˆ†è¯å™¨å¯¼å…¥ä»¥ä¸‹å‚æ•°åˆ°æ¨¡å‹é…ç½®**
- `vocab_size`: è¯æ±‡è¡¨å¤§å°
- `pad_token_id`: PAD token ID
- `bos_token_id`: BOS token ID  
- `eos_token_id`: EOS token ID

#### æ­¥éª¤ 3: é¢„è®­ç»ƒ

```bash
# ç¼–è¾‘ pretrain.sh è®¾ç½®ä½ çš„æ•°æ®é›†
# ç„¶åè¿è¡Œ
bash pretrain.sh
```

### æ–¹å¼ä¸‰ï¼šä»£ç å±‚é¢ä½¿ç”¨

```python
from train_tokenizer import train_custom_tokenizer
from custom_model import CustomModelConfig, CustomModelForCausalLM

# 1. è®­ç»ƒåˆ†è¯å™¨
tokenizer = train_custom_tokenizer(
    input_files=['data1.txt', 'data2.txt'],
    output_dir='./my_tokenizer',
    vocab_size=32000
)

# 2. ä»åˆ†è¯å™¨å¯¼å…¥å‚æ•°
vocab_size = tokenizer.vocab_size
pad_token_id = tokenizer.pad_token_id
bos_token_id = tokenizer.bos_token_id
eos_token_id = tokenizer.eos_token_id

# 3. åˆ›å»ºæ¨¡å‹é…ç½®ï¼ˆä½¿ç”¨åˆ†è¯å™¨å‚æ•°ï¼‰
config = CustomModelConfig(
    vocab_size=vocab_size,  # ä»åˆ†è¯å™¨å¯¼å…¥
    pad_token_id=pad_token_id,  # ä»åˆ†è¯å™¨å¯¼å…¥
    bos_token_id=bos_token_id,  # ä»åˆ†è¯å™¨å¯¼å…¥
    eos_token_id=eos_token_id,  # ä»åˆ†è¯å™¨å¯¼å…¥
    hidden_size=768,
    num_hidden_layers=12,
)

# 4. åˆå§‹åŒ–æ¨¡å‹
model = CustomModelForCausalLM(config)

# 5. ä¿å­˜
model.save_pretrained('./my_model')
tokenizer.save_pretrained('./my_model')
```

## ğŸ“ æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | ç”¨é€” |
|------|------|
| `train_tokenizer.py` | è®­ç»ƒ BPE åˆ†è¯å™¨ |
| `custom_model.py` | è‡ªå®šä¹‰æ¨¡å‹å®šä¹‰ï¼ˆTransformer è§£ç å™¨ï¼‰ |
| `init_weights.py` | åˆå§‹åŒ–æ¨¡å‹ï¼ˆä»åˆ†è¯å™¨å¯¼å…¥å‚æ•°ï¼‰ |
| `register_model.py` | æ³¨å†Œæ¨¡å‹åˆ° ms-swift |
| `prepare_pretrain_data.py` | é¢„è®­ç»ƒæ•°æ®å‡†å¤‡å·¥å…· |
| `pretrain.sh` | é¢„è®­ç»ƒè„šæœ¬ï¼ˆä½¿ç”¨ `swift pt`ï¼‰ |
| `sft.sh` | å¾®è°ƒè„šæœ¬ |
| `infer.sh` | æ¨ç†è„šæœ¬ |
| `run_full_example.sh` | ä¸€é”®è¿è¡Œå®Œæ•´æµç¨‹ |
| `test_model_only.py` | æµ‹è¯•æ¨¡å‹åŸºç¡€åŠŸèƒ½ |
| `README.md` | è¯¦ç»†æ–‡æ¡£ |

## ğŸ” éªŒè¯æµç¨‹

### æµ‹è¯• 1: æ¨¡å‹å®šä¹‰æµ‹è¯•ï¼ˆä¸éœ€è¦åˆ†è¯å™¨ï¼‰

```bash
python test_model_only.py
```

è¿™ä¸ªæµ‹è¯•ä¼šéªŒè¯ï¼š
- âœ“ æ¨¡å‹åˆ›å»º
- âœ“ å‰å‘ä¼ æ’­
- âœ“ æŸå¤±è®¡ç®—
- âœ“ æ–‡æœ¬ç”Ÿæˆ
- âœ“ æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
- âœ“ åˆ†è¯å™¨å‚æ•°å¯¼å…¥æµç¨‹ï¼ˆæ¨¡æ‹Ÿï¼‰

### æµ‹è¯• 2: å®Œæ•´æµç¨‹æµ‹è¯•ï¼ˆéœ€è¦åˆ†è¯å™¨ï¼‰

```bash
python test_workflow.py
```

è¿™ä¸ªæµ‹è¯•ä¼šéªŒè¯ï¼š
- âœ“ åˆ†è¯å™¨è®­ç»ƒ
- âœ“ æ¨¡å‹åˆå§‹åŒ–ï¼ˆçœŸå®çš„å‚æ•°å¯¼å…¥ï¼‰
- âœ“ æ¨¡å‹å‰å‘ä¼ æ’­
- âœ“ æ–‡æœ¬ç”Ÿæˆ

## â“ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆè¦å…ˆè®­ç»ƒåˆ†è¯å™¨ï¼Ÿ

**A**: å› ä¸ºæ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°å¿…é¡»ä¸åˆ†è¯å™¨å®Œå…¨ä¸€è‡´ã€‚å…ˆè®­ç»ƒåˆ†è¯å™¨ï¼Œå†ä»åˆ†è¯å™¨å¯¼å…¥å‚æ•°ï¼Œå¯ä»¥ç¡®ä¿ä¸€è‡´æ€§ã€‚

### Q2: æˆ‘å¯ä»¥ä½¿ç”¨å·²æœ‰çš„åˆ†è¯å™¨å—ï¼Ÿ

**A**: å¯ä»¥ï¼ä½¿ç”¨ `--tokenizer_path` å‚æ•°ï¼š

```bash
python init_weights.py \
    --tokenizer_path /path/to/existing/tokenizer \
    --output_dir ./my_model
```

### Q3: æˆ‘æƒ³ç”¨ä¸åŒçš„è¯æ±‡è¡¨å¤§å°æ€ä¹ˆåŠï¼Ÿ

**A**: é‡æ–°è®­ç»ƒåˆ†è¯å™¨ï¼š

```bash
python train_tokenizer.py \
    --vocab_size 50000 \
    --output_dir ./new_tokenizer

python init_weights.py \
    --tokenizer_path ./new_tokenizer \
    --output_dir ./my_model
```

### Q4: è¿™ä¸ªæµç¨‹å’Œå¾®è°ƒæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A**:
- **é¢„è®­ç»ƒ**ï¼ˆæœ¬ç¤ºä¾‹ï¼‰ï¼šä»éšæœºæƒé‡å¼€å§‹ï¼Œä½¿ç”¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ–‡æœ¬ï¼Œå­¦ä¹ è¯­è¨€åŸºç¡€çŸ¥è¯†
- **å¾®è°ƒ**ï¼šåœ¨å·²æœ‰é¢„è®­ç»ƒæƒé‡åŸºç¡€ä¸Šï¼Œä½¿ç”¨æ ‡æ³¨æ•°æ®ï¼Œå­¦ä¹ ç‰¹å®šä»»åŠ¡

### Q5: å¦‚ä½•ä½¿ç”¨è‡ªå·±çš„æ•°æ®ï¼Ÿ

**A**: å‡†å¤‡çº¯æ–‡æœ¬æ–‡ä»¶ï¼ˆæˆ– JSONLï¼‰ï¼Œç„¶åï¼š

```bash
# è®­ç»ƒåˆ†è¯å™¨
python train_tokenizer.py \
    --input_files your_data.txt \
    --output_dir ./tokenizer

# åˆå§‹åŒ–æ¨¡å‹
python init_weights.py \
    --tokenizer_path ./tokenizer \
    --output_dir ./model

# ä¿®æ”¹ pretrain.sh ä¸­çš„ DATASET å‚æ•°
# ç„¶åè¿è¡Œé¢„è®­ç»ƒ
bash pretrain.sh
```

## ğŸ“ å­¦ä¹ è·¯å¾„

1. **ç¬¬ä¸€æ­¥**ï¼šè¿è¡Œ `python test_model_only.py` ç†è§£æ¨¡å‹ç»“æ„
2. **ç¬¬äºŒæ­¥**ï¼šé˜…è¯» `custom_model.py` ç†è§£ Transformer å®ç°
3. **ç¬¬ä¸‰æ­¥**ï¼šè¿è¡Œ `python train_tokenizer.py` ç†è§£åˆ†è¯å™¨è®­ç»ƒ
4. **ç¬¬å››æ­¥**ï¼šé˜…è¯» `init_weights.py` ç†è§£å‚æ•°å¯¼å…¥æµç¨‹
5. **ç¬¬äº”æ­¥**ï¼šè¿è¡Œ `bash run_full_example.sh` èµ°é€šå®Œæ•´æµç¨‹
6. **ç¬¬å…­æ­¥**ï¼šä½¿ç”¨è‡ªå·±çš„æ•°æ®è®­ç»ƒåˆ†è¯å™¨å’Œæ¨¡å‹

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [å®Œæ•´æ–‡æ¡£](README.md)
- [ms-swift å®˜æ–¹æ–‡æ¡£](https://github.com/modelscope/ms-swift)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- [BPE åˆ†è¯ç®—æ³•](https://huggingface.co/docs/tokenizers/pipeline)

## ğŸ¤ åé¦ˆ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·åœ¨ GitHub ä¸Šæ Issueã€‚
